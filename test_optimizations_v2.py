"""
âš ï¸ æ­¤æ–‡ä»¶å·²åºŸå¼ƒ (v3.2) âš ï¸

æµ‹è¯•ä¼˜åŒ–åçš„Conv-TasNetæ¨¡å‹
éªŒè¯æ‰€æœ‰ä¼˜åŒ–æ˜¯å¦æ­£å¸¸å·¥ä½œ

åºŸå¼ƒåŸå› ï¼š
    - ä½¿ç”¨æ—§ç‰ˆæ•°æ®åŠ è½½API
    - æµ‹è¯•çš„ä¼˜åŒ–åŠŸèƒ½ï¼ˆç¼“å­˜ã€å½’ä¸€åŒ–ã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼‰å·²å†…ç½®åˆ°ä¸»ä»£ç 
    - åŠŸèƒ½å·²é›†æˆåˆ° trainer/trainer.py ä¸­

æ›¿ä»£æ–¹æ¡ˆï¼š
    ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬æµ‹è¯•ä¼˜åŒ–åŠŸèƒ½
    python scripts/3_train.py --num-epochs 1
    
å»ºè®®ï¼š
    æ­¤æ–‡ä»¶å¯ä»¥åˆ é™¤æˆ–ä½œä¸ºå†å²å‚è€ƒä¿ç•™
"""

import torch
import yaml
import sys
import os
import io

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8ï¼ˆè§£å†³Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.conv_tasnet import ConvTasNet
from dataset.dataloader import create_dataloader
from utils.metrics import calculate_si_sdr, calculate_sdr, calculate_si_sdri
from trainer.trainer import Trainer
from utils.logger import setup_logger


def test_model_structure():
    """æµ‹è¯•æ¨¡å‹ç»“æ„ï¼ˆéªŒè¯ReLUæ©ç ï¼‰"""
    print("\n" + "="*80)
    print("æµ‹è¯• 1: æ¨¡å‹ç»“æ„éªŒè¯")
    print("="*80)
    
    # åˆ›å»ºæ¨¡å‹
    model = ConvTasNet(
        num_speakers=2,
        encoder_filters=512,
        encoder_kernel_size=16,
        encoder_stride=8,
        bottleneck_channels=128,
        hidden_channels=256,
        skip_channels=128,
        kernel_size=3,
        num_blocks=8,
        num_repeats=3,
        norm_type='gLN',
        causal=False
    )
    
    # æ£€æŸ¥æ©ç å±‚çš„æ¿€æ´»å‡½æ•°
    mask_activation = model.separation.mask_conv[2]
    print(f"âœ“ æ©ç æ¿€æ´»å‡½æ•°: {type(mask_activation).__name__}")
    
    if isinstance(mask_activation, torch.nn.ReLU):
        print("  âœ“ æ­£ç¡®ï¼šä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼ˆå…è®¸ä¿¡å·æ”¾å¤§ï¼‰")
    else:
        print("  âœ— é”™è¯¯ï¼šåº”è¯¥ä½¿ç”¨ReLUè€Œé", type(mask_activation).__name__)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    audio_length = 16000
    mixture = torch.randn(batch_size, audio_length)
    
    with torch.no_grad():
        separated = model(mixture)
    
    print(f"âœ“ è¾“å…¥å½¢çŠ¶: {mixture.shape}")
    print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {separated.shape}")
    
    # éªŒè¯æ©ç å¯ä»¥>1ï¼ˆä¿¡å·æ”¾å¤§ï¼‰
    with torch.no_grad():
        encoder_output = model.encoder(mixture)
        masks = model.separation(encoder_output)
    
    max_mask = masks.max().item()
    min_mask = masks.min().item()
    print(f"âœ“ æ©ç èŒƒå›´: [{min_mask:.4f}, {max_mask:.4f}]")
    
    if max_mask > 1.0:
        print(f"  âœ“ æ­£ç¡®ï¼šæ©ç å¯ä»¥ >1ï¼ˆæœ€å¤§å€¼={max_mask:.4f}ï¼‰ï¼Œæ”¯æŒä¿¡å·æ”¾å¤§")
    else:
        print(f"  âš  è­¦å‘Šï¼šæ©ç æœ€å¤§å€¼={max_mask:.4f}ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")
    
    print("\næµ‹è¯•1 é€šè¿‡ï¼\n")
    return True


def test_data_normalization():
    """æµ‹è¯•æ•°æ®å½’ä¸€åŒ–"""
    print("="*80)
    print("æµ‹è¯• 2: æ•°æ®å½’ä¸€åŒ–éªŒè¯")
    print("="*80)
    
    from dataset.dataloader import SeparationDataset
    import tempfile
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ•°æ®
    temp_dir = tempfile.mkdtemp()
    mixture_dir = os.path.join(temp_dir, 'mixture')
    clean_dir = os.path.join(temp_dir, 'clean')
    os.makedirs(mixture_dir, exist_ok=True)
    os.makedirs(clean_dir, exist_ok=True)
    
    # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘
    import torchaudio
    for i in range(2):
        # æ··åˆéŸ³é¢‘
        mixture = torch.randn(1, 16000) * 0.5  # éšæœºå¹…åº¦
        torchaudio.save(
            os.path.join(mixture_dir, f'test_{i:04d}.wav'),
            mixture, 16000
        )
        
        # å¹²å‡€éŸ³é¢‘
        s1 = torch.randn(1, 16000) * 0.3
        s2 = torch.randn(1, 16000) * 0.7
        torchaudio.save(
            os.path.join(clean_dir, f'test_{i:04d}_s1.wav'),
            s1, 16000
        )
        torchaudio.save(
            os.path.join(clean_dir, f'test_{i:04d}_s2.wav'),
            s2, 16000
        )
    
    # æµ‹è¯•å½’ä¸€åŒ–
    dataset_with_norm = SeparationDataset(
        data_dir=temp_dir,
        sample_rate=16000,
        segment_length=16000,
        use_cache=False,
        normalize=True,
        target_level=-25.0
    )
    
    dataset_without_norm = SeparationDataset(
        data_dir=temp_dir,
        sample_rate=16000,
        segment_length=16000,
        use_cache=False,
        normalize=False
    )
    
    # æ£€æŸ¥å½’ä¸€åŒ–æ•ˆæœ
    mixture_norm, sources_norm = dataset_with_norm[0]
    mixture_raw, sources_raw = dataset_without_norm[0]
    
    print(f"âœ“ åŸå§‹æ··åˆä¿¡å·å¹…åº¦èŒƒå›´: [{mixture_raw.min():.4f}, {mixture_raw.max():.4f}]")
    print(f"âœ“ å½’ä¸€åŒ–æ··åˆä¿¡å·å¹…åº¦èŒƒå›´: [{mixture_norm.min():.4f}, {mixture_norm.max():.4f}]")
    
    # éªŒè¯å½’ä¸€åŒ–æ•ˆæœ
    rms_norm = torch.sqrt(torch.mean(mixture_norm ** 2)).item()
    target_rms = 10 ** (-25.0 / 20)
    
    print(f"âœ“ å½’ä¸€åŒ–åRMS: {rms_norm:.6f}")
    print(f"âœ“ ç›®æ ‡RMS: {target_rms:.6f}")
    
    if abs(rms_norm - target_rms) < 0.01:
        print("  âœ“ æ­£ç¡®ï¼šå½’ä¸€åŒ–æˆåŠŸ")
    else:
        print(f"  âš  è­¦å‘Šï¼šRMSåå·®è¾ƒå¤§")
    
    # æ¸…ç†
    import shutil
    shutil.rmtree(temp_dir)
    
    print("\næµ‹è¯•2 é€šè¿‡ï¼\n")
    return True


def test_metrics():
    """æµ‹è¯•å¤šæŒ‡æ ‡è®¡ç®—"""
    print("="*80)
    print("æµ‹è¯• 3: å¤šæŒ‡æ ‡è®¡ç®—éªŒè¯")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•ä¿¡å·
    length = 16000
    target = torch.randn(length)
    
    # æµ‹è¯•SI-SDR
    estimation = target + torch.randn(length) * 0.1
    si_sdr = calculate_si_sdr(estimation, target)
    print(f"âœ“ SI-SDRè®¡ç®—æˆåŠŸ: {si_sdr:.2f} dB")
    
    # æµ‹è¯•SDR
    sdr = calculate_sdr(estimation, target)
    print(f"âœ“ SDRè®¡ç®—æˆåŠŸ: {sdr:.2f} dB")
    
    # æµ‹è¯•SI-SDRi
    mixture = target + torch.randn(length)
    si_sdri = calculate_si_sdri(estimation, target, mixture)
    print(f"âœ“ SI-SDRiè®¡ç®—æˆåŠŸ: {si_sdri:.2f} dB")
    
    print("\næµ‹è¯•3 é€šè¿‡ï¼\n")
    return True


def test_trainer_config():
    """æµ‹è¯•è®­ç»ƒå™¨é…ç½®ï¼ˆå­¦ä¹ ç‡ç­–ç•¥ï¼‰"""
    print("="*80)
    print("æµ‹è¯• 4: è®­ç»ƒå™¨é…ç½®éªŒè¯")
    print("="*80)
    
    # åŠ è½½é…ç½®
    with open('config/config.yml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹: {config['training']['scheduler']['type']}")
    print(f"âœ“ åˆå§‹å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"âœ“ å‡åŠpatience: {config['training']['scheduler']['patience']}")
    print(f"âœ“ å‡åŠå› å­: {config['training']['scheduler']['factor']}")
    print(f"âœ“ æœ€å°å­¦ä¹ ç‡: {config['training']['scheduler']['min_lr']}")
    
    if config['training']['scheduler']['type'] == 'Halving':
        print("  âœ“ æ­£ç¡®ï¼šä½¿ç”¨Halvingç­–ç•¥ï¼ˆè®ºæ–‡æ ‡å‡†ï¼‰")
    else:
        print(f"  âš  æç¤ºï¼šå½“å‰ä½¿ç”¨{config['training']['scheduler']['type']}ç­–ç•¥")
    
    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨æµ‹è¯•
    model = ConvTasNet.from_config(config)
    logger = setup_logger('test', 'experiments/test/logs')
    
    try:
        trainer = Trainer(model, config, logger, device='cpu')
        print("âœ“ è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥è°ƒåº¦å™¨ç±»å‹
        scheduler_type = type(trainer.scheduler).__name__
        print(f"âœ“ è°ƒåº¦å™¨å®ä¾‹: {scheduler_type}")
        
        if scheduler_type == 'ReduceLROnPlateau':
            print("  âœ“ æ­£ç¡®ï¼šä½¿ç”¨ReduceLROnPlateauï¼ˆHalvingç­–ç•¥ï¼‰")
        
        # æ£€æŸ¥ä¼˜åŒ–å™¨é…ç½®
        optimizer = trainer.optimizer
        print(f"âœ“ ä¼˜åŒ–å™¨: {type(optimizer).__name__}")
        print(f"âœ“ ä¼˜åŒ–å™¨å‚æ•°:")
        print(f"  - lr: {optimizer.param_groups[0]['lr']}")
        print(f"  - betas: {optimizer.param_groups[0]['betas']}")
        print(f"  - eps: {optimizer.param_groups[0]['eps']}")
        print(f"  - weight_decay: {optimizer.param_groups[0]['weight_decay']}")
        
        if optimizer.param_groups[0]['weight_decay'] == 0:
            print("  âœ“ æ­£ç¡®ï¼šæ— æƒé‡è¡°å‡ï¼ˆè®ºæ–‡æ ‡å‡†ï¼‰")
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒå™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    print("\næµ‹è¯•4 é€šè¿‡ï¼\n")
    return True


def test_complete_pipeline():
    """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    print("="*80)
    print("æµ‹è¯• 5: å®Œæ•´æµç¨‹éªŒè¯ï¼ˆå°è§„æ¨¡ï¼‰")
    print("="*80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ•°æ®
    train_dir = "data/processed/mixed/train"
    if not os.path.exists(train_dir):
        print("âš  è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡å®Œæ•´æµç¨‹æµ‹è¯•")
        print(f"  è¯·å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬")
        return True
    
    # åŠ è½½é…ç½®
    with open('config/config.yml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºå°è§„æ¨¡æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    try:
        print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        train_loader = create_dataloader(
            data_dir=train_dir,
            batch_size=1,
            num_workers=0,
            sample_rate=config['dataset']['sample_rate'],
            segment_length=config['dataset']['segment_length'],
            shuffle=False,
            use_cache=False,
            normalize=True,  # å¯ç”¨å½’ä¸€åŒ–
            target_level=-25.0,
            augmentation=False,  # æµ‹è¯•æ—¶å…³é—­å¢å¼º
            dynamic_mixing=False
        )
        print(f"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œbatchæ•°: {len(train_loader)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        mixtures, sources = next(iter(train_loader))
        print(f"âœ“ æ•°æ®å½¢çŠ¶:")
        print(f"  - æ··åˆä¿¡å·: {mixtures.shape}")
        print(f"  - å¹²å‡€ä¿¡å·: {sources.shape}")
        
        # åˆ›å»ºæ¨¡å‹
        print("\nåˆ›å»ºæ¨¡å‹...")
        model = ConvTasNet.from_config(config)
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            separated = model(mixtures)
        print(f"âœ“ åˆ†ç¦»ä¿¡å·å½¢çŠ¶: {separated.shape}")
        
        # è®¡ç®—æŒ‡æ ‡
        print("\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        from utils.metrics import evaluate_separation
        metrics_result = evaluate_separation(
            model, 
            train_loader, 
            device='cpu',
            metrics=['si_sdr', 'sdr']
        )
        print(f"âœ“ SI-SDR: {metrics_result['si_sdr']:.2f} dB")
        print(f"âœ“ SDR: {metrics_result['sdr']:.2f} dB")
        
        print("\nâœ“ å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âœ— å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\næµ‹è¯•5 é€šè¿‡ï¼\n")
    return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*80)
    print(" Conv-TasNet ä¼˜åŒ–éªŒè¯æµ‹è¯•")
    print("="*80)
    print("\nåŸºäºè®ºæ–‡: Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking")
    print("æµ‹è¯•é¡¹ç›®:")
    print("  1. æ¨¡å‹ç»“æ„ï¼ˆReLUæ©ç ï¼‰")
    print("  2. æ•°æ®å½’ä¸€åŒ–")
    print("  3. å¤šæŒ‡æ ‡è®¡ç®—")
    print("  4. è®­ç»ƒå™¨é…ç½®ï¼ˆHalvingç­–ç•¥ï¼‰")
    print("  5. å®Œæ•´æµç¨‹")
    print("\n" + "="*80 + "\n")
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("æ¨¡å‹ç»“æ„", test_model_structure()))
    results.append(("æ•°æ®å½’ä¸€åŒ–", test_data_normalization()))
    results.append(("å¤šæŒ‡æ ‡è®¡ç®—", test_metrics()))
    results.append(("è®­ç»ƒå™¨é…ç½®", test_trainer_config()))
    results.append(("å®Œæ•´æµç¨‹", test_complete_pipeline()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print(" æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*80)
    
    for name, passed in results:
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"{name:20s}: {status}")
    
    all_passed = all(result[1] for result in results)
    
    print("="*80)
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¼˜åŒ–å·²æˆåŠŸå®æ–½ã€‚")
        print("\nå»ºè®®ä¸‹ä¸€æ­¥:")
        print("  1. è¿è¡Œå®Œæ•´è®­ç»ƒ: python scripts/3_train.py")
        print("  2. ç›‘æ§è®­ç»ƒæ›²çº¿å’ŒæŒ‡æ ‡")
        print("  3. å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½")
    else:
        print("\nâš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
    print("\n")


if __name__ == "__main__":
    main()

