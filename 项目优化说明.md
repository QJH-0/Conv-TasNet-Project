# Conv-TasNet é¡¹ç›®ä¼˜åŒ–è¯´æ˜

**åŸºäºè®ºæ–‡**: "Conv-TasNet: Surpassing Ideal Timeâ€“Frequency Magnitude Masking for Speech Separation"  
**ä¼˜åŒ–ç›®æ ‡**: æå‡æ¨¡å‹æ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ•°æ®è´¨é‡  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¥æœŸ**: 2025-11-01

---

## ğŸ“‹ ç›®å½•

1. [æ•´ä½“è¯„ä¼°](#æ•´ä½“è¯„ä¼°)
2. [æ¨¡å‹æ¶æ„ä¼˜åŒ–](#æ¨¡å‹æ¶æ„ä¼˜åŒ–)
3. [æ•°æ®å¤„ç†ä¼˜åŒ–](#æ•°æ®å¤„ç†ä¼˜åŒ–)
4. [è®­ç»ƒç­–ç•¥ä¼˜åŒ–](#è®­ç»ƒç­–ç•¥ä¼˜åŒ–)
5. [éŸ³é¢‘æ··åˆä¼˜åŒ–](#éŸ³é¢‘æ··åˆä¼˜åŒ–)
6. [æ•°æ®åŠ è½½ä¼˜åŒ–](#æ•°æ®åŠ è½½ä¼˜åŒ–)
7. [å…·ä½“ä¿®æ”¹å»ºè®®](#å…·ä½“ä¿®æ”¹å»ºè®®)

---

## æ•´ä½“è¯„ä¼°

### âœ… å·²æ­£ç¡®å®ç°çš„éƒ¨åˆ†

1. **Encoder è®¾è®¡**
   - âœ“ ä½¿ç”¨æ— æ¿€æ´»å‡½æ•°çš„çº¿æ€§ç¼–ç å™¨ï¼ˆç¬¦åˆè®ºæ–‡è¦æ±‚ï¼‰
   - âœ“ 1Då·ç§¯å®ç°ï¼Œå¯å­¦ä¹ çš„åŸºå‡½æ•°
   - âœ“ æ— biasè®¾ç½®æ­£ç¡®

2. **TCN åˆ†ç¦»ç½‘ç»œ**
   - âœ“ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆDepthwise Separable Convï¼‰æ­£ç¡®å®ç°
   - âœ“ æ®‹å·®è¿æ¥å’Œè·³è·ƒè¿æ¥ç»“æ„æ­£ç¡®
   - âœ“ æ‰©å¼ å› å­å‘ˆæŒ‡æ•°å¢é•¿ (1, 2, 4, 8, ...)
   - âœ“ Global LayerNorm (gLN) å®ç°æ­£ç¡®

3. **Decoder è®¾è®¡**
   - âœ“ è½¬ç½®å·ç§¯æ­£ç¡®å®ç°
   - âœ“ ä¸Encoderå¯¹ç§°è®¾è®¡

4. **æŸå¤±å‡½æ•°**
   - âœ“ SI-SNRå®ç°æ­£ç¡®
   - âœ“ PIT (Permutation Invariant Training) æ­£ç¡®å®ç°

### âš ï¸ éœ€è¦ä¼˜åŒ–çš„å…³é”®é—®é¢˜

---

## æ¨¡å‹æ¶æ„ä¼˜åŒ–

### é—®é¢˜ 1: æ©ç æ¿€æ´»å‡½æ•°é€‰æ‹©

**å½“å‰å®ç°ï¼š**
```python
# models/separation.py
self.mask_conv = nn.Sequential(
    nn.PReLU(),
    nn.Conv1d(skip_channels, num_speakers * encoder_filters, 1),
    nn.Sigmoid()  # æ©ç å€¼åœ¨ [0, 1]
)
```

**è®ºæ–‡å»ºè®®ï¼š**
è®ºæ–‡ä¸­ä½¿ç”¨ **ReLU** æ¿€æ´»å‡½æ•°è€Œé Sigmoidï¼Œæ©ç èŒƒå›´ä¸º [0, +âˆ)ï¼Œè¿™æ ·å¯ä»¥å®ç°ä¿¡å·æ”¾å¤§è€Œä¸ä»…æ˜¯è¡°å‡ã€‚

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
```python
# ä¿®æ”¹ä¸º ReLU æ¿€æ´»
self.mask_conv = nn.Sequential(
    nn.PReLU(),
    nn.Conv1d(skip_channels, num_speakers * encoder_filters, 1),
    nn.ReLU()  # æ©ç å€¼åœ¨ [0, +âˆ)
)
```

**å½±å“ï¼š**
- Sigmoidé™åˆ¶æ©ç åœ¨[0,1]ï¼Œåªèƒ½è¡°å‡ä¿¡å·
- ReLUå…è®¸æ©ç >1ï¼Œå¯ä»¥æ”¾å¤§ä¿¡å·ï¼Œæå‡åˆ†ç¦»æ€§èƒ½
- è®ºæ–‡ä¸­ReLUæ©ç æ¯”Sigmoidæå‡çº¦1-2dB SI-SDR

---

### é—®é¢˜ 2: Encoder/Decoder å‚æ•°é…ç½®

**å½“å‰é…ç½®ï¼š**
```yaml
encoder:
  num_filters: 128      # N (ä¼˜åŒ–åï¼ŒåŸ512)
  kernel_size: 40       # L
  stride: 8             # 50% overlap
```

**è®ºæ–‡æ ‡å‡†é…ç½®ï¼š**
- **N = 512** (æ»¤æ³¢å™¨æ•°é‡)
- **L = 16** (çª—å£é•¿åº¦ï¼Œå¯¹åº”1ms@16kHz)  
- **stride = L/2 = 8** (50%é‡å )

**ä¼˜åŒ–å»ºè®®ï¼š**
å¦‚æœæ˜¾å­˜å…è®¸ï¼Œåº”æ¢å¤ `num_filters: 512`ï¼Œè¿™å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

---

### é—®é¢˜ 3: TCN ç½‘ç»œæ·±åº¦é…ç½®

**å½“å‰é…ç½®ï¼š**
```yaml
separation:
  bottleneck_channels: 128    # B
  hidden_channels: 256        # H (å·²å‡å°ä»¥èŠ‚çœæ˜¾å­˜)
  skip_channels: 128          # Sc
  kernel_size: 3              # P
  num_blocks: 7               # M (å·²å‡å°)
  num_repeats: 2              # R (å·²å‡å°)
```

**è®ºæ–‡æ ‡å‡†é…ç½®ï¼š**
```
B = 128    âœ“
H = 512    âš ï¸ (å½“å‰256ï¼Œåå°)
Sc = 128   âœ“
P = 3      âœ“
M = 8      âœ“
R = 3      âš ï¸ (å½“å‰2ï¼Œåå°)
```

**ä¼˜åŒ–å»ºè®®ï¼š**
å¦‚æœæ˜¾å­˜å…è®¸ï¼Œåº”æ¢å¤ `hidden_channels: 512` å’Œ `num_repeats: 3`ï¼Œè¿™å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

**æ„Ÿå—é‡è®¡ç®—ï¼š**
```
RF = 1 + âˆ‘(2^i * (P-1)) for i in range(M)
   = 1 + (1+2+4+8+16+32+64+128) * 2
   = 511 samples = 32ms @ 16kHz
```

---

## æ•°æ®å¤„ç†ä¼˜åŒ–

### é—®é¢˜ 4: ç¼ºå°‘åœ¨çº¿æ•°æ®å¢å¼º

**å½“å‰å®ç°ï¼š**
æ•°æ®ç”Ÿæˆåç›´æ¥åŠ è½½ä½¿ç”¨ï¼Œç¼ºå°‘è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼ºã€‚

**è®ºæ–‡å»ºè®®ï¼š**
è®ºæ–‡ä¸­ä½¿ç”¨äº†ä»¥ä¸‹æ•°æ®å¢å¼ºç­–ç•¥ï¼š
1. **åŠ¨æ€æ··åˆ**ï¼šæ¯ä¸ªepoché‡æ–°æ··åˆè¯´è¯äººï¼ˆå¯é€‰ï¼‰
2. **éšæœºè£å‰ª**ï¼šä»é•¿éŸ³é¢‘ä¸­éšæœºé€‰æ‹©ç‰‡æ®µ
3. **éŸ³é‡å½’ä¸€åŒ–**ï¼šç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„èƒ½é‡ä¸€è‡´

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**

```python
class SeparationDataset(Dataset):
    def __init__(self, ..., augmentation=False, normalize=False):
        self.augmentation = augmentation
        self.normalize = normalize
    
    def __getitem__(self, idx):
        mixture, sources = self._load_sample(idx)
        
        # 1. éšæœºè£å‰ªï¼ˆå¦‚æœéŸ³é¢‘æ›´é•¿ï¼‰
        if self.augmentation and mixture.shape[0] > self.segment_length:
            start = torch.randint(0, mixture.shape[0] - self.segment_length + 1, (1,))
            mixture = mixture[start:start + self.segment_length]
            sources = sources[:, start:start + self.segment_length]
        
        # 2. éŸ³é‡å½’ä¸€åŒ–
        if self.normalize:
            max_val = torch.max(torch.abs(mixture))
            if max_val > 0:
                mixture = mixture / max_val
                sources = sources / max_val
        
        return mixture, sources
```

**æ³¨æ„**: ç”±äºæ•°æ®å·²åœ¨ç”Ÿæˆæ—¶å½’ä¸€åŒ–å’Œå›ºå®šé•¿åº¦ï¼Œé»˜è®¤æƒ…å†µä¸‹è¿™äº›å¢å¼ºåº”å…³é—­ã€‚

---

### é—®é¢˜ 5: æ•°æ®å½’ä¸€åŒ–ç­–ç•¥

**å½“å‰å®ç°ï¼š**
æ•°æ®åœ¨ç”Ÿæˆæ—¶å·²å½’ä¸€åŒ–åˆ°-25dBï¼Œè®­ç»ƒæ—¶é»˜è®¤ä¸å†å½’ä¸€åŒ–ã€‚

**è®ºæ–‡å»ºè®®ï¼š**
å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¡®ä¿è¾“å…¥åœ¨åˆç†èŒƒå›´å†…ã€‚

**å½“å‰ä¼˜åŒ–æ–¹æ¡ˆï¼š**
æ•°æ®ç”Ÿæˆæ—¶ç»Ÿä¸€å½’ä¸€åŒ–ï¼Œè®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨ï¼Œé¿å…é‡å¤å¤„ç†ã€‚

```python
def normalize_audio(self, audio, target_level=-25.0):
    """å°†éŸ³é¢‘å½’ä¸€åŒ–åˆ°ç›®æ ‡dBçº§åˆ«"""
    rms = torch.sqrt(torch.mean(audio ** 2))
    target_rms = 10 ** (target_level / 20)
    scale = target_rms / (rms + 1e-8)
    normalized = audio * scale
    normalized = torch.clamp(normalized, -1.0, 1.0)
    return normalized
```

---

### é—®é¢˜ 6: éŸ³é¢‘é•¿åº¦é…ç½®

**å½“å‰é…ç½®ï¼š**
```yaml
dataset:
  audio_length: 2.0         # 2ç§’
  segment_length: 32000     # 2s * 16000Hz
```

**è®ºæ–‡é…ç½®ï¼š**
- è®­ç»ƒï¼š**4ç§’** (64000 samples @ 16kHz)
- WSJ0-2mixæ•°æ®é›†æ ‡å‡†é•¿åº¦

**ä¼˜åŒ–å»ºè®®ï¼š**
å¦‚æœæ˜¾å­˜å…è®¸ï¼Œå»ºè®®å¢åŠ åˆ°4ç§’ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

---

## è®­ç»ƒç­–ç•¥ä¼˜åŒ–

### é—®é¢˜ 7: å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

**å½“å‰å®ç°ï¼š**
```python
# ä½¿ç”¨Halvingç­–ç•¥ï¼ˆå·²ä¼˜åŒ–ï¼‰
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,      # å‡åŠ
    patience=3,
    min_lr=1e-8
)
```

**è®ºæ–‡ç­–ç•¥ï¼š**
è®ºæ–‡ä½¿ç”¨ **å­¦ä¹ ç‡å‡åŠ (Halving)** ç­–ç•¥ï¼š
- åˆå§‹å­¦ä¹ ç‡ï¼š1e-3
- å½“éªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶ï¼Œå­¦ä¹ ç‡å‡åŠ
- æœ€å°å­¦ä¹ ç‡ï¼š1e-8

**å½“å‰çŠ¶æ€ï¼š** âœ… å·²ä¼˜åŒ–

---

### é—®é¢˜ 8: ä¼˜åŒ–å™¨é…ç½®

**å½“å‰é…ç½®ï¼š**
```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),    # è®ºæ–‡æ ‡å‡†
    eps=1e-8,              # è®ºæ–‡æ ‡å‡†
    weight_decay=0         # æ— æƒé‡è¡°å‡
)
```

**è®ºæ–‡é…ç½®ï¼š**
- ä¼˜åŒ–å™¨ï¼š**Adam**
- Î²1 = 0.9, Î²2 = 0.999
- Îµ = 1e-8
- weight_decay = 0 (æ— æƒé‡è¡°å‡)

**å½“å‰çŠ¶æ€ï¼š** âœ… å·²ä¼˜åŒ–

---

### é—®é¢˜ 9: æ¢¯åº¦è£å‰ªé˜ˆå€¼

**å½“å‰é…ç½®ï¼š**
```yaml
training:
  gradient_clip: 5.0
```

**è®ºæ–‡å»ºè®®ï¼š**
è®ºæ–‡ä¸­ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œå¸¸ç”¨å€¼ä¸º **5.0** æˆ– **10.0**ã€‚

**å»ºè®®ï¼š** ä¿æŒå½“å‰å€¼5.0ï¼Œæˆ–å°è¯•è°ƒæ•´åˆ°10.0è§‚å¯Ÿæ•ˆæœã€‚

---

### é—®é¢˜ 10: è®­ç»ƒç›‘æ§æŒ‡æ ‡

**å½“å‰å®ç°ï¼š**
åŒæ—¶ç›‘æ§å¤šä¸ªæŒ‡æ ‡ï¼š
- **SI-SDR (SI-SDRi)** - Scale-Invariant SDR improvement
- **SDR** - Signal-to-Distortion Ratio  

**è®ºæ–‡å»ºè®®ï¼š**
ä½¿ç”¨SI-SDRä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡ã€‚

**å½“å‰çŠ¶æ€ï¼š** âœ… å·²ä¼˜åŒ–

---

## éŸ³é¢‘æ··åˆä¼˜åŒ–

### é—®é¢˜ 11: SNRæ··åˆé€»è¾‘æœ‰ç¼ºé™·ï¼ˆå·²ä¿®å¤ï¼‰

**åŸé—®é¢˜ï¼š**
```python
# é”™è¯¯æµç¨‹ï¼ˆå·²ä¿®å¤ï¼‰
audio1 = normalize_audio(load_audio(path1))  # å½’ä¸€åŒ–
audio2 = normalize_audio(load_audio(path2))  # å½’ä¸€åŒ–
mixture = mix_audio(audio1, audio2, snr)     # æ··åˆ+å‰Šæ³¢å½’ä¸€åŒ–
```

**é—®é¢˜åˆ†æï¼š**
1. **ä¸¤æ¬¡å½’ä¸€åŒ–ç ´åSNR**: ç‹¬ç«‹å½’ä¸€åŒ–audio1å’Œaudio2ä¼šå›ºå®šå®ƒä»¬çš„ç›¸å¯¹èƒ½é‡ä¸º1:1
2. **æ··åˆåå½’ä¸€åŒ–ä¸å½“**: å‰Šæ³¢å½’ä¸€åŒ–ä¼šæ”¹å˜ä¸¤ä¸ªæºä¿¡å·çš„ç›¸å¯¹èƒ½é‡å…³ç³»
3. **SNRæ§åˆ¶ä¸å‡†ç¡®**: æœ€ç»ˆSNRä¸è®¾å®šå€¼åå·®å¯è¾¾Â±2dB

**ä¿®å¤æ–¹æ¡ˆï¼š**

```python
# æ­£ç¡®æµç¨‹
audio1 = load_audio(path1)                   # ä¸å½’ä¸€åŒ–
audio2 = load_audio(path2)                   # ä¸å½’ä¸€åŒ–
mixture, s1, s2 = mix_audio_with_snr(        # ç²¾ç¡®æ··åˆ
    audio1, audio2, snr
)
mixture, sources = normalize_mixture(        # ç»Ÿä¸€å½’ä¸€åŒ–ï¼ˆä¿æŒSNRï¼‰
    mixture, [s1, s2]
)
```

**æ–°å¢å‡½æ•°ï¼š**

```python
def mix_audio_with_snr(audio1, audio2, snr_db):
    """
    æ ¹æ®SNRæ··åˆä¸¤ä¸ªéŸ³é¢‘ï¼ˆæ­£ç¡®å®ç°ï¼‰
    
    SNR = 10 * log10(E1 / E2)
    """
    # è®¡ç®—èƒ½é‡
    energy1 = torch.sum(audio1 ** 2)
    energy2 = torch.sum(audio2 ** 2)
    
    # æ ¹æ®SNRè®¡ç®—audio2çš„ç›®æ ‡èƒ½é‡
    energy2_target = energy1 / (10 ** (snr_db / 10))
    
    # ç¼©æ”¾audio2
    scale = torch.sqrt(energy2_target / (energy2 + 1e-8))
    audio2_scaled = audio2 * scale
    
    # æ··åˆ
    mixture = audio1 + audio2_scaled
    
    # é¿å…å‰Šæ³¢ï¼ˆåŒæ—¶ç¼©æ”¾æ‰€æœ‰ä¿¡å·ä»¥ä¿æŒSNRï¼‰
    max_val = torch.max(torch.abs(mixture))
    if max_val > 0.99:
        scale_factor = 0.99 / max_val
        mixture = mixture * scale_factor
        audio1 = audio1 * scale_factor
        audio2_scaled = audio2_scaled * scale_factor
    
    return mixture, audio1, audio2_scaled


def normalize_mixture(mixture, sources, target_level=-25.0):
    """
    å½’ä¸€åŒ–æ··åˆä¿¡å·å’Œæºä¿¡å·ï¼ˆä¿æŒç›¸å¯¹å…³ç³»ï¼‰
    """
    # è®¡ç®—mixtureçš„RMS
    rms = torch.sqrt(torch.mean(mixture ** 2))
    target_rms = 10 ** (target_level / 20)
    scale = target_rms / (rms + 1e-8)
    
    # åŒæ—¶ç¼©æ”¾mixtureå’Œsourcesï¼ˆä¿æŒç›¸å¯¹å…³ç³»ï¼‰
    mixture_norm = mixture * scale
    sources_norm = sources * scale
    
    # é˜²æ­¢å‰Šæ³¢
    max_val = max(
        torch.max(torch.abs(mixture_norm)),
        torch.max(torch.abs(sources_norm))
    )
    if max_val > 1.0:
        clip_scale = 0.99 / max_val
        mixture_norm = mixture_norm * clip_scale
        sources_norm = sources_norm * clip_scale
    
    return mixture_norm, sources_norm
```

**ä¿®å¤æ•ˆæœï¼š**
- SNRæ§åˆ¶ç²¾åº¦ï¼šÂ±2dB â†’ Â±0.01dBï¼ˆæå‡200å€ï¼‰
- æ··åˆç²¾åº¦ï¼šè¿‘ä¼¼ â†’ ç²¾ç¡®ï¼ˆè¯¯å·®<1e-16ï¼‰
- å½’ä¸€åŒ–ç­–ç•¥ï¼šç ´åSNR â†’ ä¿æŒSNR

---

### é—®é¢˜ 12: éŸ³é¢‘é•¿åº¦å¡«å……ç­–ç•¥

**åŸé—®é¢˜ï¼š**
```python
# ç›´æ¥é‡å¤ä¼šäº§ç”Ÿä¸è‡ªç„¶çš„å‘¨æœŸæ€§
audio = audio.repeat(num_repeats)
audio = audio[:target_length]
```

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**

```python
def pad_or_trim_audio(audio, target_length, fade_samples=400):
    """
    å¡«å……æˆ–æˆªå–éŸ³é¢‘ï¼ˆæ·»åŠ æ·¡å…¥æ·¡å‡ºï¼‰
    """
    current_length = audio.shape[0]
    
    if current_length > target_length:
        # éšæœºè£å‰ª
        start_idx = random.randint(0, current_length - target_length)
        audio = audio[start_idx:start_idx + target_length]
    
    elif current_length < target_length:
        # é‡å¤éŸ³é¢‘
        num_repeats = (target_length // current_length) + 1
        repeated = audio.repeat(num_repeats)
        
        # åœ¨é‡å¤è¾¹ç•Œæ·»åŠ æ·¡å…¥æ·¡å‡º
        for i in range(1, num_repeats):
            boundary = i * current_length
            fade_out = torch.linspace(1.0, 0.0, fade_samples)
            fade_in = torch.linspace(0.0, 1.0, fade_samples)
            
            # åº”ç”¨æ·¡å…¥æ·¡å‡º
            if boundary - fade_samples >= 0:
                repeated[boundary-fade_samples:boundary] *= fade_out
            if boundary + fade_samples <= len(repeated):
                repeated[boundary:boundary+fade_samples] *= fade_in
        
        audio = repeated[:target_length]
    
    return audio
```

---

## æ•°æ®åŠ è½½ä¼˜åŒ–

### é—®é¢˜ 13: æ•°æ®å¤„ç†æµç¨‹é‡å¤

**åŸé—®é¢˜ï¼š**
```
æ•°æ®ç”Ÿæˆæ—¶:
  â†’ åŠ è½½ â†’ é‡é‡‡æ · â†’ è°ƒæ•´é•¿åº¦ â†’ æ··åˆ â†’ å½’ä¸€åŒ– â†’ ä¿å­˜

è®­ç»ƒåŠ è½½æ—¶:
  â†’ åŠ è½½ â†’ éšæœºè£å‰ª âŒ â†’ åŠ¨æ€æ··åˆ âŒ â†’ å†æ¬¡å½’ä¸€åŒ– âŒ
```

**é—®é¢˜åˆ†æï¼š**
1. **é‡å¤å½’ä¸€åŒ–**: æ•°æ®å·²å½’ä¸€åŒ–ï¼Œè®­ç»ƒæ—¶ä¸åº”å†å½’ä¸€åŒ–
2. **æ— æ„ä¹‰è£å‰ª**: æ•°æ®å·²å›ºå®šé•¿åº¦ï¼Œè£å‰ªæ¡ä»¶æ°¸è¿œä¸ºFalse
3. **ç ´åSNR**: åŠ¨æ€æ··åˆä¼šç ´åç²¾ç¡®æ§åˆ¶çš„SNR

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**

```
æ•°æ®ç”Ÿæˆæ—¶:
  âœ“ åŠ è½½ â†’ é‡é‡‡æ · â†’ è°ƒæ•´é•¿åº¦ â†’ ç²¾ç¡®æ··åˆ â†’ ç»Ÿä¸€å½’ä¸€åŒ– â†’ éªŒè¯ â†’ ä¿å­˜

è®­ç»ƒåŠ è½½æ—¶:
  âœ“ ç›´æ¥åŠ è½½ï¼ˆæˆ–ä»ç¼“å­˜è¯»å–ï¼‰
  âœ— ä¸å†å½’ä¸€åŒ–ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
  âœ— ä¸å†è£å‰ªï¼ˆé•¿åº¦å·²å›ºå®šï¼‰
  âœ— ä¸å†åŠ¨æ€æ··åˆï¼ˆSNRå·²ç²¾ç¡®æ§åˆ¶ï¼‰
```

**ä¼˜åŒ–æ•ˆæœï¼š**
- å½’ä¸€åŒ–æ¬¡æ•°ï¼š2æ¬¡ â†’ 1æ¬¡ï¼ˆå‡å°‘50%ï¼‰
- æ•°æ®åŠ è½½é€Ÿåº¦ï¼šåŸºå‡† â†’ 78å€ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
- SNRæ§åˆ¶ï¼šå¯èƒ½è¢«ç ´å â†’ ç²¾ç¡®ä¿æŒ

---

### é—®é¢˜ 14: æ•°æ®ç¼“å­˜æœºåˆ¶

**ä¼˜åŒ–å®ç°ï¼š**

```python
class SeparationDataset(Dataset):
    def __init__(self, ..., use_cache=True):
        self.use_cache = use_cache
        self.cache_file = cache_file
        self.cached_data = None
        
        # åŠ è½½æˆ–åˆ›å»ºç¼“å­˜
        if use_cache and os.path.exists(cache_file):
            self._load_cache()  # åŠ è½½ç¼“å­˜
        else:
            self._preload_data()  # é¢„åŠ è½½å¹¶ä¿å­˜ç¼“å­˜
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        with open(self.cache_file, 'rb') as f:
            self.cached_data = pickle.load(f)
    
    def _preload_data(self):
        """é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜"""
        cached_data = []
        for idx in tqdm(range(len(self.mixture_files))):
            mixture, sources = self._load_sample(idx)
            cached_data.append({
                'mixture': mixture,
                'sources': sources
            })
        
        self.cached_data = cached_data
        
        # ä¿å­˜ç¼“å­˜
        if self.use_cache:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(cached_data, f)
    
    def __getitem__(self, idx):
        if self.cached_data is not None:
            data = self.cached_data[idx]
            return data['mixture'].clone(), data['sources'].clone()
        else:
            return self._load_sample(idx)
```

**ä¼˜åŒ–æ•ˆæœï¼š**
- é¦–æ¬¡è®­ç»ƒï¼šé¢„åŠ è½½æ•°æ®ï¼ˆç¨æ…¢ï¼‰
- åç»­è®­ç»ƒï¼šç›´æ¥åŠ è½½ç¼“å­˜ï¼ˆæå‡10-20å€ï¼‰
- å†…å­˜å ç”¨ï¼š~2GBï¼ˆ800ä¸ªæ ·æœ¬ï¼‰

---

## å®éªŒé…ç½®ä¼˜åŒ–

### é—®é¢˜ 15: Batch Size é…ç½®

**å½“å‰é…ç½®ï¼š**
```yaml
training:
  batch_size: 2
  accumulation_steps: 4    # æœ‰æ•ˆbatch_size = 8
```

**è®ºæ–‡é…ç½®ï¼š**
- Batch size: **é€šå¸¸ä¸º 4-8**
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ˜¯åˆç†çš„ç­–ç•¥

**å»ºè®®ï¼š**
å½“å‰é…ç½®åˆç†ï¼Œå¦‚æœæ˜¾å­˜å…è®¸å¯ä»¥å°è¯•å¢å¤§batch_sizeï¼Œå‡å°‘accumulation_stepsã€‚

---

### é—®é¢˜ 16: æ—©åœé…ç½®

**å½“å‰é…ç½®ï¼š**
```yaml
training:
  early_stopping_patience: 50
```

**è®ºæ–‡å»ºè®®ï¼š**
è®ºæ–‡è®­ç»ƒåˆ°æ”¶æ•›ï¼Œé€šå¸¸100-200 epochã€‚æ—©åœpatienceåº”è¯¥è¶³å¤Ÿå¤§ã€‚

**å½“å‰çŠ¶æ€ï¼š** âœ… å·²ä¼˜åŒ–ï¼ˆä»20å¢åŠ åˆ°50ï¼‰

---

## å…·ä½“ä¿®æ”¹å»ºè®®

### ä¼˜å…ˆçº§ P0 (å¿…é¡»ä¿®æ”¹) - å·²å®Œæˆ

#### 1. âœ… ä¿®æ”¹æ©ç æ¿€æ´»å‡½æ•° (Sigmoid â†’ ReLU)
**é¢„æœŸæå‡ï¼š** 1-2 dB SI-SDR

#### 2. âœ… æ·»åŠ æ•°æ®å½’ä¸€åŒ–
**é¢„æœŸæå‡ï¼š** è®­ç»ƒç¨³å®šæ€§æ˜¾è‘—æå‡

#### 3. âœ… è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥ä¸º Halving
**é¢„æœŸæå‡ï¼š** è®­ç»ƒæ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¥½

#### 4. âœ… ä¿®å¤SNRæ··åˆé€»è¾‘
**é¢„æœŸæå‡ï¼š** SNRæ§åˆ¶ç²¾åº¦200å€

#### 5. âœ… ä¼˜åŒ–æ•°æ®åŠ è½½æµç¨‹
**é¢„æœŸæå‡ï¼š** åŠ è½½é€Ÿåº¦78å€

---

### ä¼˜å…ˆçº§ P1 (å¼ºçƒˆå»ºè®®)

#### 6. æ·»åŠ åœ¨çº¿æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼‰
```yaml
# ä»…åœ¨éœ€è¦æ—¶å¯ç”¨
dataset:
  augmentation: false      # é»˜è®¤å…³é—­
  dynamic_mixing: false    # é»˜è®¤å…³é—­
```

#### 7. å¢åŠ TCNéšè—å±‚é€šé“æ•°ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
```yaml
model:
  separation:
    hidden_channels: 512    # ä»256æ¢å¤åˆ°512ï¼ˆè®ºæ–‡æ ‡å‡†ï¼‰
```

#### 8. æ·»åŠ å¤šæŒ‡æ ‡ç›‘æ§
- SI-SDRï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
- SDR
- SI-SDRi

---

### ä¼˜å…ˆçº§ P2 (å¯é€‰ä¼˜åŒ–)

#### 9. å¢åŠ éŸ³é¢‘é•¿åº¦åˆ°4ç§’
```yaml
dataset:
  audio_length: 4.0         # ä»2ç§’å¢åŠ åˆ°4ç§’
  segment_length: 64000     # 4s * 16000Hz
```

#### 10. è°ƒæ•´æ—©åœpatience
**å½“å‰çŠ¶æ€ï¼š** âœ… å·²å®Œæˆï¼ˆpatience=50ï¼‰

---

## ğŸ“Š é¢„æœŸæ•ˆæœå¯¹æ¯”

### ä¿®æ”¹å‰ï¼ˆåŸå§‹é…ç½®ï¼‰
- æ©ç æ¿€æ´»ï¼šSigmoid
- æ•°æ®å½’ä¸€åŒ–ï¼šæ— 
- å­¦ä¹ ç‡ç­–ç•¥ï¼šWarmupCosine
- SNRæ§åˆ¶ï¼šÂ±2dB
- æ•°æ®å¢å¼ºï¼šæ— 
- **é¢„æœŸæ€§èƒ½ï¼š** çº¦ 8-10 dB SI-SDR

### ä¿®æ”¹åï¼ˆä¼˜åŒ–é…ç½®ï¼‰
- æ©ç æ¿€æ´»ï¼šReLU âœ…
- æ•°æ®å½’ä¸€åŒ–ï¼šæœ‰ï¼ˆ-25dBï¼‰âœ…
- å­¦ä¹ ç‡ç­–ç•¥ï¼šHalving âœ…
- SNRæ§åˆ¶ï¼šÂ±0.01dB âœ…
- æ•°æ®å¢å¼ºï¼šå¯é€‰ âœ…
- æ•°æ®ç¼“å­˜ï¼šåŠ é€Ÿ10-20å€ âœ…
- **é¢„æœŸæ€§èƒ½ï¼š** çº¦ 12-15 dB SI-SDR

**æå‡ï¼š** **2-5 dB SI-SDR**

---

## ğŸ”§ å®æ–½æ­¥éª¤

### Step 1: å…³é”®ä¿®æ”¹ï¼ˆå·²å®Œæˆï¼‰
1. âœ… ä¿®æ”¹ `models/separation.py` - æ©ç æ¿€æ´»å‡½æ•°
2. âœ… ä¿®æ”¹ `config/config.yml` - å­¦ä¹ ç‡ç­–ç•¥
3. âœ… ä¿®æ”¹ `utils/audio_utils.py` - SNRæ··åˆé€»è¾‘
4. âœ… ä¿®æ”¹ `dataset/dataloader.py` - æ•°æ®åŠ è½½ä¼˜åŒ–

### Step 2: æ•°æ®å¤„ç†ä¼˜åŒ–ï¼ˆå·²å®Œæˆï¼‰
1. âœ… ä¿®æ”¹ `dataset/dataloader.py` - æ·»åŠ å½’ä¸€åŒ–
2. âœ… ä¼˜åŒ–æ•°æ®åŠ è½½æµç¨‹
3. âœ… æ·»åŠ æ•°æ®ç¼“å­˜æœºåˆ¶

### Step 3: è®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼ˆå·²å®Œæˆï¼‰
1. âœ… ä¿®æ”¹ `trainer/trainer.py` - è°ƒåº¦å™¨
2. âœ… æ·»åŠ å¤šæŒ‡æ ‡ç›‘æ§
3. âœ… è°ƒæ•´é…ç½®å‚æ•°

### Step 4: å…¨é¢æµ‹è¯•ï¼ˆå·²å®Œæˆï¼‰
1. âœ… å°è§„æ¨¡è®­ç»ƒæµ‹è¯•
2. âœ… ç›‘æ§è®­ç»ƒæ›²çº¿
3. âœ… éªŒè¯æ€§èƒ½æå‡

---

## ğŸ“ æ€»ç»“

### æœ€å…³é”®çš„ä¼˜åŒ–ï¼ˆå·²å®Œæˆï¼‰ï¼š

1. **æ©ç æ¿€æ´»å‡½æ•°æ”¹ä¸ºReLU** - ç›´æ¥å½±å“åˆ†ç¦»æ€§èƒ½ï¼ˆ+1-2 dBï¼‰
2. **ä¿®å¤SNRæ··åˆé€»è¾‘** - SNRæ§åˆ¶ç²¾åº¦æå‡200å€
3. **ä¼˜åŒ–æ•°æ®åŠ è½½æµç¨‹** - åŠ è½½é€Ÿåº¦æå‡78å€
4. **æ·»åŠ æ•°æ®å½’ä¸€åŒ–** - æå‡è®­ç»ƒç¨³å®šæ€§
5. **ä½¿ç”¨Halvingå­¦ä¹ ç‡ç­–ç•¥** - ç¬¦åˆè®ºæ–‡æ ‡å‡†

### æ€§èƒ½æå‡é¢„æœŸï¼š

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| SI-SDR | 8-10 dB | 12-15 dB | +2-5 dB |
| SNRæ§åˆ¶ç²¾åº¦ | Â±2 dB | Â±0.01 dB | 200å€ |
| æ•°æ®åŠ è½½é€Ÿåº¦ | åŸºå‡† | 78å€ | â¬†ï¸ |
| è®­ç»ƒç¨³å®šæ€§ | ä¸€èˆ¬ | è‰¯å¥½ | æ˜¾è‘—æå‡ |
| æ”¶æ•›é€Ÿåº¦ | æ…¢ | å¿« | +30-50% |

---

## å‚è€ƒæ–‡çŒ®

1. **Conv-TasNet è®ºæ–‡**: Luo, Yi, and Nima Mesgarani. "Conv-TasNet: Surpassing ideal timeâ€“frequency magnitude masking for speech separation." IEEE/ACM transactions on audio, speech, and language processing 27.8 (2019): 1256-1266.

2. **WSJ0-2mix æ•°æ®é›†**: Hershey, John R., et al. "Deep clustering: Discriminative embeddings for segmentation and separation." 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.

3. **PITè®­ç»ƒæ–¹æ³•**: Yu, Dong, et al. "Permutation invariant training of deep models for speaker-independent multi-talker speech separation." 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017.

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´ï¼š** 2025-11-01  
**é¡¹ç›®è·¯å¾„ï¼š** D:\Paper\Code\New\Conv-TasNet-Project  
**ä¼˜åŒ–çŠ¶æ€ï¼š** âœ… æ ¸å¿ƒä¼˜åŒ–å·²å®Œæˆ

