# Conv-TasNet 项目配置文件

# 数据集配置
dataset:
  name: "AISHELL-3"
  raw_data_path: "D:\\Paper\\datasets\\AISHELL-3"
  processed_data_path: "data/processed1000_20db"
  
  # wsj0-2mix / Libri2Mix 标准数据集路径
  # 注意：Windows路径使用双反斜杠 \\ 或正斜杠 /
  data_path: "data/processed1000"
#  data_path: "D:/Paper/datasets/Libri2Mix_8k"

  # 数据生成参数
  num_speakers: 20                    # 选择的说话人数量
  samples_per_speaker: 50            # 每位说话人的语音数量
  train_ratio: 0.8                    # 训练集比例
  dev_ratio: 0.1                      # 验证集比例 (dev)
  sample_rate: 8000                   # 采样率 (8kHz for wsj0-2mix/Libri2Mix_8k)
  snr_range: [-3,3]          # WSJ0-2mix / Libri2Mix 标准

  # 进阶：极限测试（仅验证集）
  # snr_range_train: [0, 20]
  # snr_range_val:   [-3, 3]
  audio_length_range: [2.0, 6.0]      # 音频长度范围（秒）[最小值, 最大值]
  # audio_length: 4.0                 # 固定音频长度（秒）- 被 audio_length_range 替代
  
  # 数据加载参数（wsj0-2mix标准格式）
  chunk_size: 32000                   # 动态切片长度（samples）= 4s * 8000Hz
  use_all_chunks: true               # 是否使用所有切片（True: 每个样本切成多个chunk，数据量翻倍；False: 每个样本只使用一个chunk）
  
  # wsj0-2mix 标准目录结构
  # 训练集: {data_path}/train/
  # 验证集: {data_path}/dev/
  # 测试集: {data_path}/test/
  # 元数据: {data_path}/metadata/

# 模型配置
model:
  name: "Conv-TasNet"
  encoder:
    num_filters: 128                  # N: 编码器滤波器数量 (减少以节省显存)
    kernel_size: 40                   # L: 卷积核大小
    stride: 8                         # 步长（50%重叠）
  
  separation:
    num_speakers: 2                   # C: 说话人数量
    bottleneck_channels: 128          # B: 瓶颈层通道数
    hidden_channels: 256              # H: 隐藏层通道数 (减少以节省显存)
    skip_channels: 128                # Sc: 跳跃连接通道数
    kernel_size: 3                    # P: 卷积核大小
    num_blocks: 7                     # M: 每个TCN块的层数 (减少以节省显存)
    num_repeats: 2                    # R: TCN块重复次数
    norm_type: "gLN"                  # 归一化类型（gLN/cLN/BN）
    causal: false                     # 是否因果卷积
  
  decoder:
    num_filters: 128                  # N: 编码器滤波器数量 (减少以节省显存)
    kernel_size: 40                   # 卷积核大小
    stride: 8                         # 步长

# 训练配置
training:
  batch_size: 2                       # 提升batch size（配合梯度累积）
  num_epochs: 10
  learning_rate: 0.001
  optimizer: "Adam"
  weight_decay: 1.0e-5                #权重衰减（L2正则化）
  
  # 优化配置
  accumulation_steps: 4               # 梯度累积步数（有效batch_size=2*4=8）
  use_amp: true                       # 启用混合精度训练（节省显存，提升速度）
  
  # 学习率调度器（论文标准：Halving策略）
  scheduler:
    type: "Halving"                   # 使用Halving策略（论文标准）
    mode: "min"                       # 监控验证损失最小值
    patience: 3                       # 3个epoch无改进则学习率减半
    factor: 0.5                       # 学习率衰减因子（减半）
    min_lr: 1.0e-8                    # 最小学习率（论文标准）
    
  gradient_clip: 200                  # 梯度裁剪阈值
  early_stopping_patience: 20         # 早停轮数（增加以避免过早停止）
  seed: 42                            # 随机种子
  resume_checkpoint: "D:/Paper/Code/Conv-TasNet-Project/experiments/exp_2000/checkpoints/checkpoint_epoch_50"             # 恢复训练的检查点路径 (null表示从头开始)
  
# 损失函数
loss:
  type: "SI-SNR"                      # Scale-Invariant SNR
  use_pit: true                       # 使用 PIT (Permutation Invariant Training)
  
# 评估配置
evaluation:
  # 评估指标配置（使用 Asteroid + PySTOI）
  metrics:
    enabled: ['si_sdr']  # 训练时只用SI-SDR，速度最快
    # 完整评估: ['si_sdr', 'sdr', 'sir', 'sar']
    # 可选: 'si_sdr', 'sdr', 'sir', 'sar', 'stoi', 'si_sdri'
    
  use_asteroid: true                  # 使用 Asteroid 库（自动 PIT）
  include_stoi: false                 # 是否计算 STOI（可懂度，纯Python）
  
  # 可视化配置
  plot_metrics: ['si_sdr']  # 在图表中显示的指标
  
# 验证配置
validation:
  batch_size: 1                       # 减少batch size以适应显存
  
# 设备配置
device:
  use_cuda: true
  gpu_ids: [0]
  num_workers: 4                      # 多进程数据加载（建议4-8，根据CPU核心数调整）
                                       # 注意：Windows需要在 if __name__ == "__main__" 下运行
  
# 日志与保存
logging:
  log_dir: "experiments/exp_1000_test/logs"
  checkpoint_dir: "experiments/exp_1000_test/checkpoints"
  checkpoint_dir_load: "experiments/exp_1000_test/checkpoints/best_model.pth"
  result_dir: "experiments/exp_1000_test/results"
  save_interval: 5                    # 每5个epoch保存一次
  visualize: true                     # 是否可视化
  tensorboard: true                   # 是否使用tensorboard
