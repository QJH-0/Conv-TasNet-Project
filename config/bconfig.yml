# BTCN (Binary Conv-TasNet) 项目配置文件 - 策略2
# 二值化策略: 深度卷积 + TCN的1×1卷积
# 二值化率: 约94%, 模型大小: 19.22 MB → 1.65 MB (11.68x压缩)

# 数据集配置
dataset:
  name: "AISHELL-3"
  raw_data_path: "D:\\Paper\\datasets\\AISHELL-3"
  processed_data_path: "data/processed3"
  
  # wsj0-2mix / Libri2Mix 标准数据集路径
  # 注意：Windows路径使用双反斜杠 \\ 或正斜杠 /
  data_path: "data/processed3"
#  data_path: "D:/Paper/datasets/Libri2Mix_8k"

  # 数据生成参数
  num_speakers: 50                    # 选择的说话人数量
  samples_per_speaker: 50            # 每位说话人的语音数量
  train_ratio: 0.8                    # 训练集比例
  dev_ratio: 0.1                      # 验证集比例 (dev)
  sample_rate: 8000                   # 采样率 (8kHz for wsj0-2mix/Libri2Mix_8k)
  snr_range: [-3, 3]                  # 信噪比范围（dB）
  audio_length_range: [2.0, 6.0]      # 音频长度范围（秒）[最小值, 最大值]
  
  # 数据加载参数（wsj0-2mix标准格式）
  chunk_size: 32000                   # 动态切片长度（samples）= 4s * 8000Hz
  use_all_chunks: true               # 是否使用所有切片（True: 每个样本切成多个chunk，数据量翻倍；False: 每个样本只使用一个chunk）

# 模型配置
model:
  name: "BTCN"  # Binary Conv-TasNet
  encoder:
    num_filters: 128                  # N: 编码器滤波器数量 (降低以减小模型)
    kernel_size: 40                   # L: 卷积核大小
    stride: 8                         # 步长（50%重叠）
  
  separation:
    num_speakers: 2                   # C: 说话人数量
    bottleneck_channels: 128          # B: 瓶颈层通道数
    hidden_channels: 256              # H: 隐藏层通道数 (降低以减小模型)
    skip_channels: 128                # Sc: 跳跃连接通道数
    kernel_size: 3                    # P: 卷积核大小
    num_blocks: 7                     # M: 每个TCN块的层数
    num_repeats: 2                    # R: TCN块重复次数
    norm_type: "gLN"                  # 归一化类型（gLN/cLN/BN）
    causal: false                     # 是否因果卷积
  
  decoder:
    num_filters: 128                  # N: 编码器滤波器数量
    kernel_size: 40                   # 卷积核大小
    stride: 8                         # 步长

# 训练配置
training:
  batch_size: 2                       # Batch size (二值化模型可以用更大的batch size)
  num_epochs: 50                     # 训练轮数 (二值化模型可能需要更多epoch)
  learning_rate: 0.001                # 初始学习率
  optimizer: "Adam"                   # 优化器
  weight_decay: 1.0e-5                # 权重衰减（L2正则化）
  
  # 优化配置
  accumulation_steps: 2               # 梯度累积步数（有效batch_size=4*2=8）
  use_amp: true                       # 启用混合精度训练（节省显存，提升速度）
  
  # 学习率调度器
  scheduler:
    type: "Halving"                   # 使用Halving策略（论文标准）
    mode: "min"                       # 监控验证损失最小值
    patience: 3                       # 3个epoch无改进则学习率减半
    factor: 0.5                       # 学习率衰减因子（减半）
    min_lr: 1.0e-8                    # 最小学习率
    
  gradient_clip: 5.0                  # 梯度裁剪阈值 (二值化网络可能需要更小的梯度裁剪)
  early_stopping_patience: 20         # 早停轮数
  seed: 42                            # 随机种子
  resume_checkpoint: "D:/Paper/Code/Conv-TasNet-Project/experiments/btcn_001/checkpoints/checkpoint_epoch_20.pth"             # 恢复训练的检查点路径 (null表示从头开始)
  
# 损失函数
loss:
  type: "SI-SNR"                      # Scale-Invariant SNR
  use_pit: true                       # 使用 PIT (Permutation Invariant Training)
  
  # 知识蒸馏配置 (暂时不启用)
  distillation:
    enabled: false                    # 是否启用知识蒸馏
    teacher_checkpoint: null          # 教师模型检查点路径
    temperature: 1.0                  # 蒸馏温度
    
    # 损失权重 (λ1-λ5)
    weights:
      spec_loss: 0.2                  # λ1: 输出波形蒸馏损失
      encoder_loss: 0.1               # λ2: 编码器输出蒸馏损失
      tcn_loss: 0.1                   # λ3: TCN特征图平均蒸馏损失
      mask_loss: 0.2                  # λ4: 掩码蒸馏损失
      task_loss: 0.4                  # λ5: 任务损失 (SI-SNR)
    
    # 权重调整策略
    weight_schedule:
      type: "dynamic"                 # dynamic: 动态调整, static: 固定权重
      initial_task_weight: 0.2        # 初始任务损失权重
      final_task_weight: 0.8          # 最终任务损失权重
      warmup_epochs: 50               # 权重调整的预热轮数
  
# 验证配置
validation:
  batch_size: 2                       # 验证batch size
  metrics: ["SI-SDR", "SDR", "SAR", "SIR"]  # 评估指标
  
# 设备配置
device:
  use_cuda: true
  gpu_ids: [0]
  num_workers: 4                      # 多进程数据加载
  
# 日志与保存
logging:
  log_dir: "experiments/btcn_001/logs"
  checkpoint_dir: "experiments/btcn_001/checkpoints"
  result_dir: "experiments/btcn_001/results"
  save_interval: 5                    # 每5个epoch保存一次
  visualize: true                     # 是否可视化
  tensorboard: true                   # 是否使用tensorboard

